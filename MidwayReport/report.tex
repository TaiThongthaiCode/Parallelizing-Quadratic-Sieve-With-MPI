\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{subfigure,indentfirst}
% for url
\usepackage{hyperref}
% for underlined text
\usepackage[normalem]{ulem}

% use some packages for importing figures of different types
% pdfig is one for importing .pdf files.  sadly, they are not all
% compatible, so you often have to convert figures to the same type.
%\usepackage{pdffig}
\usepackage{graphicx}


% this starts the document
\begin{document}

\title{CS87 Midway Progress Report: MPI and CUDA parallelization of the Quadratic Sieve Algoritihm}

\author{Tarang Saluja, Tai Thongthai \\
Computer Science Department, Swarthmore College, Swarthmore, PA  19081}

\maketitle

\section{Project Schedule/Milestones}


% An updated project schedule with milestones and dates annotated with
% information indicating which parts you have completed, which parts you are
% currently working, and which parts you have yet to start.. Be specific about
% what you are going to do (or have done), what you have left to do, and in what
% order you plan to do these things.

\begin{itemize}
 \item Week 9, Nov. 1: \textbf{What we planned}: Find all necessary benchmark values for computation (how many elements in factor base, how many elements to find. etc.) and write pseudocode for sequential implementation with actual values. \textbf{What we did}: We were able to find most necessary benchmark values for computation, but our sieving interval is still quite rough. We have only one constant value for all sizes, and is catered towards larger size numbers. Finding estimations for the sieving interval may involve more research, but we can also just take the sieving interval provided in the MPQS implementation from our bibliography. This part was completed together.
 \item Week 10, Nov. 8: \textbf{What we planned}: Implement a sequential version of Quadratic Sieve. \textbf{What we did}: During this week we were only successful in implementing the first part of our sequential implementation.
 \item Week 11, Nov. 15: \textbf{What we planned}: Implement MPI parallelization with boss/worker and then MPI scatter/reduce or MR library. \textbf{What we did}: We were able to have our sequential implementation until the point where we have a large sparse matrix to solve over GF(2). This part was completed together.
 \item Week 12, Nov. 22: Insert placeholder for sparse matrix solver over GF(2) and then parallelize with MPI Boss/Worker. In particular, each worker will receive a sieving inteval, perform sieving, and send the boss relations back after finding a certain amount of relations. We will do this part together. We will then run tests on both the sequential version and the parallel version. For the sparse matrix solver, we will use the libraries that you provided, and then we will use the Boss/Worker model that is in the MPQS implementation.
 \item Week 13, Nov. 29: Parallelize with MPI scatter and MPI reduce with MPIMR library. This will resemble MapReduce rather than a Boss/Worker relationship. Run tests. It will follow the MapReduce model that we have seen in class, and is motivated by the suggestions that are given in the paper by Asbrink and Brynnielson.
 \item Week 14, Dec. 06: Add CUDA to MPI. In particular, have different computers call CUDA while sieving to speed-up the process. This involves taking the sieving intervals given to each computer and splitting them up among CUDA kernels for faster computation. Add this to both the boss/worker parallelization and the MPIMR parallelization. Run tests so that all 4 can be compared.
 \item Week 15, Dec. 13: Add any other parallelisms for fun. This includes using CUDA while setting up our factor base as done in the paper by Pavan and perhaps using a MPI/CUDA implementation of Gaussian elimination for the matrix step. Report and compile data from all previous tests. We will make tables which show how our speedup alters with regards to number of partitions and method of parallelizing (or lack thereof).
\end{itemize}

\section {Difficulties}
The main difficulties that we have encountered so far are the size of the numbers, the difficulty of the sequential implementation, and other circumstances out of our control. We dealt with the large numbers by using a multiprecision library called GMP, which likely stores our large numbers as arrays of strings, and has its own syntax for operations. While this has simplified our task greatly, it was difficult to figure out how exactly we could employ this library. Even though we were able to intuitively understand the algorithm, it was much harder to implement it than we thought. Eventually, we were able to figure out how we can implement the algorithm, with much help from the MPQS implementation that is in our bibliography. However, we are afraid that there might still be some inefficiencies in our algorithm, especially with regards to the size of the sieving interval and the coarse granularity of how we are deciding the size of factor base. We will try to fix these inefficiences by doing more reserach into the theory, but even if we are not able to do this in addition to the other work we plan, we will still be able to have a project which compares the impact of different kinds of parallelization. Finally, Tarang broke into hives 10 days straight and Tai had to go to New York to record an acoustic version of his new single, Residue (https://open.spotify.com/album/1DIjvyoB1odcvnTeMjJh0f). We fixed this with  Zyrtec, hydrocortisone cream, and Amtrak. We will make up for this over the next few weeks by working a little bit extra.

\end{document}
