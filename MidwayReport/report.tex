\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{subfigure,indentfirst}
% for url
\usepackage{hyperref}
% for underlined text
\usepackage[normalem]{ulem}

% use some packages for importing figures of different types
% pdfig is one for importing .pdf files.  sadly, they are not all
% compatible, so you often have to convert figures to the same type.
%\usepackage{pdffig}
\usepackage{graphicx}


% this starts the document
\begin{document}

\title{CS87 Midway Progress Report: MPI and CUDA parallelization of the Quadratic Sieve Algoritihm}

\author{Tarang Saluja, Tai Thongthai \\
Computer Science Department, Swarthmore College, Swarthmore, PA  19081}

\maketitle

\section{Project Schedule/Milestones}


% An updated project schedule with milestones and dates annotated with
% information indicating which parts you have completed, which parts you are
% currently working, and which parts you have yet to start.. Be specific about
% what you are going to do (or have done), what you have left to do, and in what
% order you plan to do these things.

\begin{itemize}
 \item Week 9, Nov. 1: \textbf{What we planned}: Find all necessary benchmark values for computation (how many elements in factor base, how many elements to find. etc.) and write pseudocode for sequential implementation with actual values. \textbf{What we did}: We were able to find most necessary benchmark values for computation, but our sieving interval is still quite rough. We have only one constant value for all sizes, and is catered towards larger size numbers. Finding estimations for the sieving interval may involve more research, but we can also just take the sieving interval provided in the MPQS implementation from our bibliography. This part was completed together.
 \item Week 10, Nov. 8: \textbf{What we planned}: Implement a sequential version of Quadratic Sieve. \textbf{What we did}: During this week we were only successful in implementing the first part of our sequential implementation. This was the part where we created our factor base, which we did by ourselves. We took an implementation of the Shanks-Tonelli algorithm from the MPQS implementation that is in our bibliography. 
 \item Week 11, Nov. 15: \textbf{What we planned}: Implement MPI parallelization with boss/worker and then MPI scatter/reduce or MR library. \textbf{What we did}: We were able to have our sequential implementation until the point where we have a large sparse matrix to solve over GF(2). This part was completed together, and we implemented it on our own. 
 \item Week 12, Nov. 22: Insert placeholder for sparse matrix solver over GF(2) and then parallelize with MPI Boss/Worker. In particular, each worker will receive a sieving inteval, perform sieving, and send the boss relations back after finding a certain amount of relations. We will do this part together. We will then run tests on both the sequential version and the parallel version. While we hope to have test results, it is more likely that we will only figure out how large is feasible. However, since tests can be run in parallel with working on other aspects, we can make up for this in the next week. For the sparse matrix solver, we will use the libraries that you provided, and then we will use the Boss/Worker model that is in the MPQS implementation. However, as we don't have multiple polynomials, we will adapt it for our own purposes. 
 \item Week 13, Nov. 29: Parallelize with MPI scatter and MPI reduce with MPIMR library. This will resemble MapReduce rather than a Boss/Worker relationship. Run tests. It will follow the MapReduce model that we have seen in class, and it is motivated by the suggestions that are given in the paper by Asbrink and Brynnielson.
 \item Week 14, Dec. 06: Add CUDA to MPI. In particular, have different computers call CUDA while sieving to speed-up the process. This involves taking the sieving intervals given to each computer and splitting them up among CUDA kernels for faster computation. Add this to both the boss/worker parallelization and the MPIMR parallelization. Run tests so that all 4 can be compared.
 \item Week 15, Dec. 13: Add any other parallelisms for fun. This includes using CUDA while setting up our factor base as done in the paper by Pavan and perhaps using a MPI/CUDA implementation of Gaussian elimination for the matrix step. Report and compile data from all previous tests. We will make tables which show how our speedup alters with regards to number of partitions and method of parallelizing (or lack thereof).
\end{itemize}

\section {Difficulties}
The main difficulties that we have encountered so far are the size of the numbers, the difficulty of the sequential implementation, and other circumstances out of our control.We dealt with the large numbers by using a multiprecision library called GMP, which likely stores our large numbers as arrays of strings, and has its own syntax for operations. While this has simplified our task greatly, it was difficult to figure out how exactly we could employ this library and move between the mpz_t types and integers in an effective and correct way. Even though we were able to intuitively understand the algorithm, it was much harder to implement it than we thought. When we had stepped through it earlier, we would often do it with an example, but implementing in general required theoretical notions that we did had not thought about carefully. For example, what is the most effective way to deal with dividing by powers of primes (we decided to only look at primes and divide multiple times in a step). Eventually, we were able to figure out how we can implement the algorithm,and this was with much help from the MPQS implementation that is in our bibliography. However, we are afraid that there might still be some inefficiencies in our algorithm, especially with regards to the size of the sieving interval, the coarse granularity of how we are deciding the size of factor base, and the way that we check to see whether a number is divisible by a prime during each iteration. We will try to fix these inefficiences by doing more reserach into the theory, but even if we are not able to do this in addition to the other work we plan, we will still be able to have a project which compares the impact of different kinds of parallelization. Finally, Tarang broke into hives daily for more than a week shortly after falling behind on all his classes for personal reasons. This was sort of fixed with Zyrtec and hydrocortisone cream, but as soon as Tarang could work semi-normally again, Tai went to New York to record an acoustic version if his new single, Residue (https://open.spotify.com/album/1DIjvyoB1odcvnTeMjJh0f). This was later fixed with Amtrak. This caused an unfortunate delay, but we plan to give extra hours to this project and make up for it. 

\end{document}
